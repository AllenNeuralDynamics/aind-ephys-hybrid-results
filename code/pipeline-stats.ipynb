{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7670fda5",
   "metadata": {},
   "source": [
    "### AIND Ephys Pipeline Stats\n",
    "\n",
    "This notebook processes all ephys sessions with sorted data in the AIND pipeline, counts the number of probes and units per session, and summarizes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c4bed-e30b-4c2a-ad56-6e0b077572b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "\n",
    "from aind_data_access_api.document_db import MetadataDbClient\n",
    "\n",
    "\n",
    "from utils import (\n",
    "    get_all_ecephys_derived, \n",
    "    get_duration_from_session,\n",
    "    get_unique_sessions_with_dates,\n",
    "    parse_curation_notes\n",
    ")\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bcd9dc-584c-4df2-8b02-f2fc219c9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = Path(\"../results\")\n",
    "figures_folder = results_folder / \"pipeline\"\n",
    "figures_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0072669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for database connection\n",
    "API_GATEWAY_HOST = os.environ.get(\"API_GATEWAY_HOST\", \"api.allenneuraldynamics.org\")\n",
    "DATABASE = os.environ.get(\"DATABASE\", \"metadata_index\")\n",
    "COLLECTION = os.environ.get(\"COLLECTION\", \"data_assets\")\n",
    "\n",
    "# Initialize the client\n",
    "client = MetadataDbClient(\n",
    "    host=API_GATEWAY_HOST,\n",
    "    database=DATABASE,\n",
    "    collection=COLLECTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e957a2-ba1b-41d1-a698-7c6558b151cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_deployment_date = datetime(2024, 2, 20)\n",
    "ks4_deployment_date = datetime(2024, 11, 10)\n",
    "up_to_date = datetime(2025, 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977af686-410a-45d8-aeac-17a072ec89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_derived = get_all_ecephys_derived(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d44497-8f65-4034-b667-096627885edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_assets = [d for d in all_derived if \"sorted\" in d[\"data_description\"][\"name\"]]\n",
    "sorted_asset_with_session_metadata = [d for d in sorted_assets if d[\"session\"] is not None]\n",
    "\n",
    "print(f\"Total sorted assets: {len(sorted_assets)} - with session metadata: {len(sorted_asset_with_session_metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c4e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique sessions\n",
    "unique_sessions_df = get_unique_sessions_with_dates(sorted_assets)\n",
    "unique_sessions_df['created_date'] = pd.to_datetime(unique_sessions_df['created_date'])\n",
    "print(f\"Total unique sessions: {len(unique_sessions_df)}\")\n",
    "print(f\"Date range: {unique_sessions_df['created_date'].min()} to {unique_sessions_df['created_date'].max()}\")\n",
    "\n",
    "\n",
    "# Only keep sessions after pipeline deployment\n",
    "unique_sessions_df = unique_sessions_df[unique_sessions_df['created_date'] >= pipeline_deployment_date]\n",
    "unique_sessions_df = unique_sessions_df[unique_sessions_df['created_date'] <= up_to_date]\n",
    "print(f\"\\nUnique sessions after pipeline deployment: {len(unique_sessions_df)}\")\n",
    "print(f\"Date range: {unique_sessions_df['created_date'].min()} to {unique_sessions_df['created_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42002ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_asset = unique_sessions_df.loc[unique_sessions_df['created_date'].idxmin()]\n",
    "print(f\"First asset with new pipeline:\\n{first_asset['full_name']} created on {first_asset['created_date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_assets_entries = []\n",
    "no_processing = 0\n",
    "no_processing_pipeline = 0\n",
    "no_data_processes = 0\n",
    "no_curation = 0\n",
    "\n",
    "for index, row in unique_sessions_df.iterrows():\n",
    "    sorted_asset = row[\"entry\"]\n",
    "    # create a dict for each entry with:\n",
    "    # - number of probes\n",
    "    # - number of streams\n",
    "    # - sorter\n",
    "    # - curation output\n",
    "    processing = sorted_asset.get(\"processing\", {})\n",
    "    if not processing:\n",
    "        no_processing += 1\n",
    "        no_processing_pipeline += 1\n",
    "        no_data_processes += 1\n",
    "        no_curation += 1\n",
    "        continue\n",
    "    processing_pipeline = processing.get(\"processing_pipeline\", {})\n",
    "    if not processing_pipeline:\n",
    "        no_processing_pipeline += 1\n",
    "        no_data_processes += 1\n",
    "        no_curation += 1\n",
    "        continue\n",
    "\n",
    "    data_processes = processing_pipeline.get(\"data_processes\", [])\n",
    "    # if an entry of data_processes is a list, we have to flatten it\n",
    "    flattened_data_processes = []\n",
    "    for dp in data_processes:\n",
    "        if isinstance(dp, list):\n",
    "            flattened_data_processes.extend(dp)\n",
    "        else:\n",
    "            flattened_data_processes.append(dp)\n",
    "    data_processes = flattened_data_processes\n",
    "\n",
    "    # get unique stream names and probe names using regex (from Spike sorting data_process)\n",
    "    curation_processes = [\n",
    "        dp for dp in data_processes if \"curation\" in dp[\"name\"]\n",
    "    ]\n",
    "    if len(curation_processes) == 0:\n",
    "        no_curation += 1\n",
    "        continue\n",
    "    spike_sorting_processes = [\n",
    "        dp for dp in data_processes if \"Spike sorting\" in dp[\"name\"]\n",
    "    ]\n",
    "    if len(spike_sorting_processes) == 0:\n",
    "        no_curation += 1\n",
    "        print(f\"No spike sorting process for {sorted_asset['data_description']['name']} (# curation: {len(curation_processes)})\")\n",
    "        continue\n",
    "    recording_names = [\n",
    "        dp[\"parameters\"][\"recording_name\"] for dp in curation_processes \n",
    "        if \"recording_name\" in dp[\"parameters\"]\n",
    "    ]\n",
    "    num_streams = len(set(recording_names))\n",
    "    probe_names = []\n",
    "    for recording_name in recording_names:\n",
    "        match = re.search(r'\\.Probe([A-Z]|\\d+)(?:-AP)?[_.]', recording_name)\n",
    "        if match:\n",
    "            probe_name = f\"Probe{match.group(1)}\"\n",
    "            probe_names.append(probe_name)\n",
    "    probe_names = sorted(list(set(probe_names)))\n",
    "    num_probes = len(probe_names)\n",
    "    sorter_name = spike_sorting_processes[0][\"parameters\"].get(\"sorter_name\", \"kilosort2_5\")\n",
    "    curation_outputs = {}\n",
    "    for dp in curation_processes:\n",
    "        # curation_output = dp[\"outputs\"]\n",
    "        curation_notes = dp.get(\"notes\", \"\")\n",
    "        curation_output = parse_curation_notes(curation_notes)\n",
    "            \n",
    "        for k, v in curation_output.items():\n",
    "            if k not in curation_outputs:\n",
    "                curation_outputs[k] = v\n",
    "            else:\n",
    "                curation_outputs[k] += v\n",
    "\n",
    "    entry_dict = {\n",
    "        \"name\": sorted_asset[\"data_description\"][\"name\"],\n",
    "        \"num_probes\": num_probes,\n",
    "        \"num_streams\": num_streams,\n",
    "        \"sorter\": sorter_name,\n",
    "        \"created_date\": row[\"created_date\"],\n",
    "    }\n",
    "    for k in curation_outputs:\n",
    "        entry_dict[k] = curation_outputs[k]\n",
    "\n",
    "    sorted_assets_entries.append(entry_dict)\n",
    "\n",
    "sessions_df = pd.DataFrame(sorted_assets_entries)\n",
    "\n",
    "sessions_df = sessions_df.query(\n",
    "    \"sorter in ['kilosort2_5', 'kilosort4']\"\n",
    ")\n",
    "sessions_df['created_date'] = pd.to_datetime(sessions_df['created_date'])\n",
    "sessions_df = sessions_df.sort_values('created_date')\n",
    "\n",
    "\n",
    "print(f\"Total number of assets: {len(unique_sessions_df)}\")\n",
    "print(f\"\\tNumber of sorted assets without processing info: {no_processing}\")\n",
    "print(f\"\\tNumber of sorted assets without processing pipeline info: {no_processing_pipeline}\")\n",
    "print(f\"\\tNumber of sorted assets without no curation: {no_curation}\")\n",
    "print(f\"\\tNumber of sorted assets to analyze: {len(sessions_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e946b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weekly aggregation\n",
    "sessions_df['week'] = sessions_df['created_date'].dt.to_period('W')\n",
    "weekly_counts = sessions_df.groupby('week').size().reset_index(name='count')\n",
    "weekly_counts['week_start'] = weekly_counts['week'].dt.start_time\n",
    "\n",
    "# Create cumulative count (post-pipeline only)\n",
    "sessions_df_sorted = sessions_df.sort_values('created_date').reset_index(drop=True)\n",
    "sessions_df_sorted['cumulative_count'] = range(1, len(sessions_df_sorted) + 1)\n",
    "\n",
    "print(f\"Pipeline weekly data points: {len(weekly_counts)}\")\n",
    "print(f\"Date range for pipeline data: {weekly_counts['week_start'].min()} to {weekly_counts['week_start'].max()}\")\n",
    "print(f\"Pipeline sessions: {len(sessions_df)}\")\n",
    "print(f\"\\nWeekly stats (pipeline only):\")\n",
    "print(f\"  Mean sessions per week: {weekly_counts['count'].mean():.1f}\")\n",
    "print(f\"  Max sessions in a week: {weekly_counts['count'].max()}\")\n",
    "print(f\"  Min sessions in a week: {weekly_counts['count'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9cf77-3eeb-4175-9a59-116b0a692fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df_sorted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative probe counts over time\n",
    "sessions_df_sorted['cumulative_probes'] = sessions_df_sorted['num_probes'].cumsum()\n",
    "\n",
    "# Calculate cumulative unit statistics over time\n",
    "sessions_df_sorted['cumulative_total_units'] = sessions_df_sorted['total_units'].cumsum()\n",
    "sessions_df_sorted['cumulative_passing_qc'] = sessions_df_sorted['passing_qc'].cumsum()\n",
    "sessions_df_sorted['cumulative_failing_qc'] = sessions_df_sorted['failing_qc'].cumsum()\n",
    "sessions_df_sorted['cumulative_noise_units'] = sessions_df_sorted['noise_units'].cumsum()\n",
    "sessions_df_sorted['cumulative_neural_units'] = sessions_df_sorted['neural_units'].cumsum()\n",
    "sessions_df_sorted['cumulative_sua_units'] = sessions_df_sorted['sua_units'].cumsum()\n",
    "sessions_df_sorted['cumulative_mua_units'] = sessions_df_sorted['mua_units'].cumsum()\n",
    "\n",
    "print(f\"Total cumulative probes: {sessions_df_sorted['cumulative_probes'].iloc[-1]:,.0f}\")\n",
    "print(f\"Total cumulative units: {sessions_df_sorted['cumulative_total_units'].iloc[-1]:,.0f}\")\n",
    "print(f\"Units passing QC: {sessions_df_sorted['cumulative_passing_qc'].iloc[-1]:,.0f}\")\n",
    "print(f\"Units labeled as noise: {sessions_df_sorted['cumulative_noise_units'].iloc[-1]:,.0f}\")\n",
    "print(f\"Units labeled as neural: {sessions_df_sorted['cumulative_neural_units'].iloc[-1]:,.0f}\")\n",
    "print(f\"Units labeled as SUA: {sessions_df_sorted['cumulative_sua_units'].iloc[-1]:,.0f}\")\n",
    "print(f\"Units labeled as MUA: {sessions_df_sorted['cumulative_mua_units'].iloc[-1]:,.0f}\")\n",
    "print(\"\\n\\nCumulative Rates:\")\n",
    "print(f\"QC pass rate: {sessions_df_sorted['cumulative_passing_qc'].iloc[-1] / sessions_df_sorted['cumulative_total_units'].iloc[-1] * 100:.1f}%\")\n",
    "print(f\"Neural unit rate: {sessions_df_sorted['cumulative_neural_units'].iloc[-1] / sessions_df_sorted['cumulative_total_units'].iloc[-1] * 100:.1f}%\")\n",
    "print(f\"SUA rate: {sessions_df_sorted['cumulative_sua_units'].iloc[-1] / sessions_df_sorted['cumulative_total_units'].iloc[-1] * 100:.1f}%\")\n",
    "print(f\"MUA rate: {sessions_df_sorted['cumulative_mua_units'].iloc[-1] / sessions_df_sorted['cumulative_total_units'].iloc[-1] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ee7cb-c185-49d3-b393-aa2b18047d44",
   "metadata": {},
   "source": [
    "### Session plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abb133-1884-4fe8-80f4-69b7a2647a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color-coded weekly sessions plot by sorter\n",
    "fig_counts, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# Define colors for each sorter\n",
    "sorter_colors = {\n",
    "    'kilosort2_5': 'C3',      # Blue\n",
    "    'kilosort4': 'C4',        # Red\n",
    "}\n",
    "\n",
    "# Create weekly aggregation by sorter\n",
    "sessions_df_sorted['week'] = sessions_df_sorted['created_date'].dt.to_period('W')\n",
    "weekly_sorter_counts = sessions_df_sorted.groupby(['week', 'sorter']).size().unstack(fill_value=0)\n",
    "weekly_sorter_counts['week_start'] = weekly_sorter_counts.index.to_timestamp()\n",
    "\n",
    "# Create stacked bar chart\n",
    "bottom = None\n",
    "sorter_handles = []\n",
    "\n",
    "for sorter in ['kilosort2_5', 'kilosort4']:\n",
    "    if sorter in weekly_sorter_counts.columns:\n",
    "        bars = ax.bar(weekly_sorter_counts['week_start'], \n",
    "                     weekly_sorter_counts[sorter],\n",
    "                     bottom=bottom, \n",
    "                     width=6, \n",
    "                     alpha=0.8, \n",
    "                     color=sorter_colors[sorter],\n",
    "                     label=sorter.capitalize(),\n",
    "                     edgecolor='white', \n",
    "                     linewidth=0.5)\n",
    "        sorter_handles.append(bars)\n",
    "        \n",
    "        if bottom is None:\n",
    "            bottom = weekly_sorter_counts[sorter]\n",
    "        else:\n",
    "            bottom += weekly_sorter_counts[sorter]\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Week Starting Date', fontsize=12)\n",
    "ax.set_ylabel('Number of Sessions Processed', fontsize=12)\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add grid and legend\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "ax.legend(loc='upper left', fontsize=11)\n",
    "\n",
    "\n",
    "# Add statistics\n",
    "total_sessions = len(sessions_df_sorted)\n",
    "ks25_count = len(sessions_df_sorted[sessions_df_sorted['sorter'] == 'kilosort2_5'])\n",
    "ks4_count = len(sessions_df_sorted[sessions_df_sorted['sorter'] == 'kilosort4'])\n",
    "\n",
    "stats_text = f\"\"\"Session Statistics by Sorter:\n",
    "• Kilosort2.5: {ks25_count:,} sessions ({ks25_count/total_sessions*100:.1f}%)\n",
    "• Kilosort4: {ks4_count:,} sessions ({ks4_count/total_sessions*100:.1f}%)\n",
    "• Total: {total_sessions:,} sessions\n",
    "• KS4 deployment: June 11, 2024\"\"\"\n",
    "print(stats_text)\n",
    "\n",
    "sns.despine(fig_counts)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig_counts.savefig(figures_folder / \"session_counts.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f114ad-bc02-485b-83b0-32f104632af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color-coded cumulative sessions plot by sorter (excluding spykingcircus2)\n",
    "fig_cumulative, axs = plt.subplots(figsize=(15, 5), ncols=3)\n",
    "\n",
    "ax = axs[0]\n",
    "# Calculate cumulative counts for each sorter\n",
    "for sorter in ['kilosort2_5', 'kilosort4']:\n",
    "    sorter_data = sessions_df_sorted[sessions_df_sorted['sorter'] == sorter].copy()\n",
    "    if len(sorter_data) > 0:\n",
    "        sorter_data = sorter_data.reset_index(drop=True)\n",
    "        sorter_data['cumulative_count'] = range(1, len(sorter_data) + 1)\n",
    "        \n",
    "        # Plot cumulative line for this sorter\n",
    "        ax.plot(sorter_data['created_date'], sorter_data['cumulative_count'], \n",
    "                linewidth=3, color=sorter_colors[sorter], alpha=0.8, \n",
    "                label={sorter.capitalize()})\n",
    "        final_count = len(sorter_data)\n",
    "        final_date = sorter_data['created_date'].max()\n",
    "        final_date = final_date - timedelta(days=70)\n",
    "        \n",
    "        ax.annotate(f'{final_count:,}', \n",
    "                    xy=(final_date, final_count), xytext=(10, 10),\n",
    "                    textcoords='offset points', fontsize=10, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round', facecolor=sorter_colors[sorter], alpha=0.3),\n",
    "                    color=sorter_colors[sorter])\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Number of Sessions', fontsize=12)\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "# Add grid and legend\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_axisbelow(True)\n",
    "ax.legend(loc='upper left', fontsize=11)\n",
    "\n",
    "\n",
    "# Add statistics\n",
    "ks25_sessions = len(sessions_df_sorted[sessions_df_sorted['sorter'] == 'kilosort2_5'])\n",
    "ks4_sessions = len(sessions_df_sorted[sessions_df_sorted['sorter'] == 'kilosort4'])\n",
    "ks4_days = (sessions_df_sorted['created_date'].max() - ks4_deployment_date).days\n",
    "ks25_rate = ks4_sessions / ks4_days if ks4_days > 0 else 0\n",
    "ks4_rate = ks4_sessions / ks4_days if ks4_days > 0 else 0\n",
    "\n",
    "session_stats_text = f\"\"\"Session Statistics:\n",
    "• KS2.5 total: {ks25_sessions:,} sessions\n",
    "• KS4 total: {ks4_sessions:,} sessions\n",
    "• KS4 daily rate: {ks4_rate:.1f} sessions/day\n",
    "• KS4 adoption: {ks4_sessions/(ks25_sessions+ks4_sessions)*100:.1f}% of total\"\"\"\n",
    "print(session_stats_text)\n",
    "\n",
    "# Plot cumulative probe line\n",
    "ax = axs[1]\n",
    "ax.plot(sessions_df_sorted['created_date'], sessions_df_sorted['cumulative_probes'], \n",
    "        linewidth=2.5, color='darkgreen', alpha=0.8, label='Cumulative Probes')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Number of Probes', fontsize=12)\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "# Add final count annotation\n",
    "final_probes = sessions_df_sorted['cumulative_probes'].iloc[-1]\n",
    "final_date = sessions_df_sorted['created_date'].iloc[-1] - timedelta(90)\n",
    "ax.annotate(f'{final_probes:,}', \n",
    "            xy=(final_date, final_probes), xytext=(10, 10),\n",
    "            textcoords='offset points', fontsize=10, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='green', alpha=0.3),\n",
    "            color=\"darkgreen\")\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Add statistics\n",
    "avg_probes_per_session = sessions_df_sorted['num_probes'].mean()\n",
    "probe_stats_text = f\"\"\"\\n\\nProbe Statistics:\n",
    "• Total probes processed: {final_probes:,}\n",
    "• Sessions processed: {len(sessions_df_sorted):,}\n",
    "• Average probes per session: {avg_probes_per_session:.1f}\n",
    "• Processing period: {(final_date - pipeline_deployment_date).days} days\"\"\"\n",
    "print(probe_stats_text)\n",
    "\n",
    "# Cumulative units\n",
    "ax = axs[2]\n",
    "line1 = ax.plot(sessions_df_sorted['created_date'], sessions_df_sorted['cumulative_total_units'], \n",
    "                linewidth=3, color='darkblue', alpha=0.8, label='All')\n",
    "\n",
    "# Add final count annotation\n",
    "final_total = int(sessions_df_sorted['cumulative_total_units'].iloc[-1])\n",
    "final_date = sessions_df_sorted['created_date'].iloc[-1] - timedelta(90)\n",
    "ax.annotate(f'{final_total:,}', \n",
    "            xy=(final_date, final_total), xytext=(10, 10),\n",
    "            textcoords='offset points', fontsize=10, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='blue', alpha=0.3),\n",
    "            color=\"darkblue\")\n",
    "\n",
    "# Overlay: Neural units\n",
    "line2 = ax.plot(sessions_df_sorted['created_date'], sessions_df_sorted['cumulative_neural_units'], \n",
    "                linewidth=2, color='darkorange', alpha=0.8, label='Neural')\n",
    "\n",
    "final_neural = int(sessions_df_sorted['cumulative_neural_units'].iloc[-1])\n",
    "final_date = sessions_df_sorted['created_date'].iloc[-1] - timedelta(90)\n",
    "ax.annotate(f'{final_neural:,}', \n",
    "            xy=(final_date, final_neural), xytext=(10, 10),\n",
    "            textcoords='offset points', fontsize=10, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='orange', alpha=0.3),\n",
    "            color=\"darkorange\")\n",
    "\n",
    "# Overlay: Passing QC\n",
    "line3 = ax.plot(sessions_df_sorted['created_date'], sessions_df_sorted['cumulative_passing_qc'], \n",
    "                linewidth=2, color='darkgreen', alpha=0.8, label='Passing QC')\n",
    "\n",
    "final_passing = int(sessions_df_sorted['cumulative_passing_qc'].iloc[-1])\n",
    "final_date = sessions_df_sorted['created_date'].iloc[-1] - timedelta(90)\n",
    "ax.annotate(f'{final_passing:,}', \n",
    "            xy=(final_date, final_passing), xytext=(10, 10),\n",
    "            textcoords='offset points', fontsize=10, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='green', alpha=0.3),\n",
    "            color=\"darkgreen\")\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Number of Units (M)', fontsize=12)\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper left', fontsize=11)\n",
    "\n",
    "# Final statistics\n",
    "final_total = sessions_df_sorted['cumulative_total_units'].iloc[-1]\n",
    "final_passing = sessions_df_sorted['cumulative_passing_qc'].iloc[-1]\n",
    "final_failing = sessions_df_sorted['cumulative_failing_qc'].iloc[-1]\n",
    "final_noise = sessions_df_sorted['cumulative_noise_units'].iloc[-1]\n",
    "final_neural = sessions_df_sorted['cumulative_neural_units'].iloc[-1]\n",
    "final_date = sessions_df_sorted['created_date'].iloc[-1]\n",
    "\n",
    "# Calculate percentages\n",
    "qc_pass_rate = (final_passing / final_total) * 100\n",
    "qc_fail_rate = (final_failing / final_total) * 100\n",
    "noise_rate = (final_noise / final_total) * 100\n",
    "neural_rate = (final_neural / final_total) * 100\n",
    "\n",
    "# Add comprehensive statistics\n",
    "units_stats_text = f\"\"\"\\n\\nUnit Processing Statistics:\n",
    "• Total units processed: {final_total:,.0f}\n",
    "• Units passing QC: {final_passing:,.0f} ({qc_pass_rate:.1f}%)\n",
    "• Units failing QC: {final_failing:,.0f} ({qc_fail_rate:.1f}%)\n",
    "• Noise units: {final_noise:,.0f} ({noise_rate:.1f}%)\n",
    "• Neural units: {final_neural:,.0f} ({neural_rate:.1f}%)\n",
    "• Processing period: {(final_date - pipeline_deployment_date).days} days\n",
    "• Average units per session: {final_total/len(sessions_df_sorted):,.0f}\"\"\"\n",
    "print(units_stats_text)\n",
    "\n",
    "ax.set_yticklabels([f\"{t:.1f}\" for t in np.arange(0, 2, 0.2)])\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    # Rotate x-axis labels\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Only keep date in central plot\n",
    "    if i != 1:\n",
    "        ax.set_xlabel(\"\")\n",
    "\n",
    "sns.despine(fig_cumulative)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig_cumulative.savefig(figures_folder / \"cumulative.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
